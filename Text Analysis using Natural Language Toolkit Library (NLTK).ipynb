{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 Princesses Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this link (Links to an external site.), download the 12dancingprincesses.txt file. Then read the file and use the NLTK library to tokenize each word in the text. Below is some code to help you get started with loading the file. The text in square brackets [] is where you would add your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['create an empty list here to hold the tokens at the end']\n",
    "\n",
    "#with open([the name of the file], 'r') as f:\n",
    "#    for line in f:\n",
    "#        cline = line.strip() #get rid of newline character\n",
    "#\n",
    "#    if cline == '': pass  #this will skip over lines that only had a newline and are now blank\n",
    "#    else:\n",
    "#        tknls = [write the function to tokenize the words]\n",
    "\n",
    "#    for token in tknls:\n",
    "#        [write the function to append each token to the empty list you created at the start of this code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using that code to load and tokenizing each word, then remove the punctuation and filler words (stopwords) from the list of tokens. Lastly, get the top 10 words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GBTC408007ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GBTC408007ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\GBTC408007ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GBTC408007ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\GBTC408007ur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import names  \n",
    "from string import punctuation\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('names')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = []\n",
    "\n",
    "with open('datasets/datasets_12dancingprincesses.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        cline = line.strip() #get rid of newline character\n",
    "        cline = cline.lower()\n",
    "        if cline == '': pass  #this will skip over lines that only had a newline and are now blank\n",
    "        \n",
    "        else:\n",
    "            #tknls = sent_tokenize(cline)\n",
    "            tknls = word_tokenize(cline)\n",
    "        for token in tknls:\n",
    "            listing.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'twelve',\n",
       " 'dancing',\n",
       " 'princesses',\n",
       " 'the',\n",
       " 'twelve',\n",
       " 'dancing',\n",
       " 'princesses',\n",
       " 'there',\n",
       " 'was']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "for token in listing:\n",
    "    if token in punctuation:\n",
    "        listing.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize verbs\n",
    "wnl = WordNetLemmatizer()\n",
    "lemm_v = []\n",
    "for word in listing:\n",
    "    lemm_v.append(wnl.lemmatize(word, pos='v'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'twelve',\n",
       " 'dance',\n",
       " 'princesses',\n",
       " 'the',\n",
       " 'twelve',\n",
       " 'dance',\n",
       " 'princesses',\n",
       " 'there',\n",
       " 'be']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_v[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_vn = []\n",
    "for word in lemm_v:\n",
    "    lemm_vn.append(wnl.lemmatize(word, pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'twelve',\n",
       " 'dance',\n",
       " 'princess',\n",
       " 'the',\n",
       " 'twelve',\n",
       " 'dance',\n",
       " 'princess',\n",
       " 'there',\n",
       " 'be']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_vn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['twelve',\n",
       " 'dance',\n",
       " 'princess',\n",
       " 'twelve',\n",
       " 'dance',\n",
       " 'princess',\n",
       " 'king',\n",
       " 'twelve',\n",
       " 'beautiful',\n",
       " 'daughter']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords\n",
    "eng = stopwords.words('english')\n",
    "new_words = []\n",
    "count = 0\n",
    "for token in lemm_vn:\n",
    "    if token not in eng:\n",
    "        new_words.append(token)\n",
    "    else:\n",
    "        count+=1\n",
    "print(count)\n",
    "new_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twelve dan\n"
     ]
    }
   ],
   "source": [
    "#remove apostrophe (and all non alphabetical values)\n",
    "import re\n",
    "string = ' '.join(new_words)\n",
    "print(string[:10])\n",
    "\n",
    "updated_words = re.sub(r'[^a-zA-Z ]', '', string).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twelve',\n",
       " 'dance',\n",
       " 'princess',\n",
       " 'twelve',\n",
       " 'dance',\n",
       " 'princess',\n",
       " 'king',\n",
       " 'twelve',\n",
       " 'beautiful',\n",
       " 'daughter']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 25),\n",
       " ('soldier', 19),\n",
       " ('king', 17),\n",
       " ('say', 17),\n",
       " ('go', 16),\n",
       " ('dance', 13),\n",
       " ('twelve', 12),\n",
       " ('come', 12),\n",
       " ('eldest', 11),\n",
       " ('night', 10)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find 10 most common words\n",
    "word_count = FreqDist(updated_words)\n",
    "word_count.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
